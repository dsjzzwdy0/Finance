<?xml version="1.0" encoding="UTF-8"?>
<settings>
	<!--
	  Interval (in milliseconds) to invoke a crawl thread.
	  There is an HTTP hit every <interval> millisecond.
	-->
	<interval>2000</interval>

	<!--
	  Interval (in milliseconds) to invoke a monitor thread.
	  Monitor adds new entry in the monitor.log every <monitorInterval>
          milliseconds
	-->
	<monitorInterval>5000</monitorInterval>

	<!-- HTTP connection timeout in milliseconds -->
	<connectionTimeout>30000</connectionTimeout>


	<!-- Headers to be used by the http client crawler
	<headers>
		<header name="User-Agent">Mozilla/4.0 (compatible; MSIE 5.0; Windows NT; DigExt)</header>
		<header name="Cache-Control">no-cache</header>
		<header name="Accept-Language">zh-CN</header>
	</headers>
 	-->
	<!-- 
	<header name="User-Agent">Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 6.1; WOW64; Trident/5.0; SLCC2; .NET CLR 2.0.50727; .NET CLR 3.5.30729; .NET CLR 3.0.30729; Media Center PC 6.0; .NET4.0C; .NET4.0E) </header>
	<header name="Cache-Control">no-cache</header>
	-->

	<headers>
	    <header name="Accept">text/html, */*; q=0.01</header>
		<header name="User-Agent">Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.87 Safari/537.36</header>
		<header name="Connection">keep-alive</header>
		<header name="Accept-Language">zh-CN,zh;q=0.8</header>
		<header name="Content-Type">application/x-www-form-urlencoded</header>
	</headers>
	
	<UserAgents>
		<User-Agent>Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; Trident/5.0)</User-Agent>  <!-- IE9.0 -->
		<User-Agent>Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1; Maxthon 2.0)</User-Agent>  <!-- 傲游 -->
		<User-Agent>User-Agent:Mozilla/5.0 (Macintosh; U; Intel Mac OS X 10_6_8; en-us) AppleWebKit/534.50 (KHTML, like Gecko) Version/5.1</User-Agent>  <!-- safari 5.1 – MAC  -->
		<User-Agent>Mozilla/5.0 (Windows; U; Windows NT 6.1; en-us) AppleWebKit/534.50 (KHTML, like Gecko) Version/5.1 Safari/534.50</User-Agent>  <!-- safari windows -->
	</UserAgents>
	
	
	<!-- URLs to start crawling from -->
	<crawl-urls>
		<!--<url>http://www.msn.com/</url>-->
		<url>http://www.sina.com.cn/</url>
	</crawl-urls>


	<!--
	  URL patterns (regexps!!!) to allow or deny set of URLs
	  permission=true  - these patterns are allowed (anything else is denied)
	  permission=false - these patterns are denied (anything else is allowed)
	-->
	<url-patterns permission="true">
		<pattern>.*?sina\.com.cn.*</pattern>
	</url-patterns>

</settings>
